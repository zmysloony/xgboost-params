% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc} 
\usepackage[parfill]{parskip}
\usepackage[T1]{fontenc} 

\usepackage{polski}
\usepackage{float}
\usepackage{fixltx2e}
\usepackage{calc}
\usepackage[export]{adjustbox} % also loads graphicx
\usepackage{makeidx}
\usepackage{multicol}
\usepackage{multirow}
\PassOptionsToPackage{warn}{textcomp}
\usepackage{textcomp}
\usepackage[nointegrals]{wasysym}
\usepackage[table]{xcolor}

\usepackage{csvsimple}

% Font selection
\usepackage[T1]{fontenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage{amssymb}
\usepackage{sectsty}

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round

\usepackage{graphicx} % support the \includegraphics command and options
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{graphicx}

\usepackage{ifpdf}
\ifpdf
\usepackage[pdftex,pagebackref=true]{hyperref}
\else
\usepackage[ps2pdf,pagebackref=true]{hyperref}
\fi
\hypersetup{%
	colorlinks=true,%
	urlcolor=blue,
	linkcolor=blue,%
	citecolor=blue,%
	unicode%
}


%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}
%%% END Article customizations

%%% The "real" document content comes below...

\title{Optymalizacja hiperparametrów xgboost\\
	\large Dokumentacja wstępna}

\author{Przemysław Stawczyk, Piotr Zmyślony}
\date{15 kwietnia 2020} % Activate to display a given date or no date (if empty),
% otherwise the current date is printed 

\begin{document}
	\maketitle
	\setcounter{secnumdepth}{3}
	\setcounter{tocdepth}{3}
	\tableofcontents
	\clearpage

\section{Treść zadania}
Naszym zadaniem jest przetestowanie różnych algorytmów heurystycznych/populacyjnych w kontekście problemu strojenia hiperparametrów algorytmu xgboost. Problem wyboru hiperparametrów wynika z ich bardzo dużej ilości, co często rozwiązane jest poprzez manualny dobór parametrów klasyfikatora.

Projekt zostanie zrealizowany w języku Python 3+.
\section{Dane testowe}
Jako dane na których będziemy trenować i testować klasyfikatory przyjęliśmy proponowany zestaw danych \url{https://www.kaggle.com/c/porto-seguro-safe-driver-prediction}. Zawiera on 57 atrybutów opisujących klientów firmy ubezpieczeniowej i jeden atrybut binarny sygnalizujący, czy w ciągu roku od zawarcia umowy, klient skorzystał z ubezpieczenia.

\begin{figure}[H]
	\caption{Brakujące atrybuty}
	\label{attrib_analysis}
	\centering
	\includegraphics[width=\textwidth]{attrib_analysis}
\end{figure}

\subsection{Analiza danych}
Po wstępnej analizie danych odkryliśmy, że w zbiorze danych posiadamy około 79\% niekompletnych wierszy. Rysunek \ref{attrib_analysis} przedstawia pokrycie niekompletnych atrybutów - jest ich jedynie 13, z czego większość jest wybrakowana w bardzo niewielkim stopniu.

Największym winowajcą jest atrybut binarny \textsl{ps\_car\_03\_cat}, którego brakuje aż w 70\% wierszy, oraz atrybut \textsl{ps\_car\_05\_cat} (brakuje go w 44\% przypadków).

Dodatkowo, występuje znaczna dysproporcja między klasami rekordów - tylko 3\% wierszy opisuje klientów, którzy skorzystali z ubezpieczenia. Stąd niezbędna będzie interpolacja danych, tak aby ilość rekordów obu klas była równa.

\subsection{Uzupełnienie brakujących danych}
W związku z powyższym, planujemy uzupełnić brakujące atrybuty na bazie kompletnych wierszy danych. Do tego zastosujemy bibliotekę pythonową \textsl{impyute}, ale nie będziemy analizować, jaka jest zależność między konkretnymi metodami interpolacji wybrakowanych atrybutów a hiperparametrami trenowanego klasyfikatora - ręcznie wybierzemy tą, która daje najlepsze (i najszybsze) rezultaty.

\section{Propozycja rozwiązania}
Planujemy zaimplementować następujące algorytmy:
\begin{itemize}
	\item mutacyjny algorytm wspinaczkowy z tabu. \\ 
	W 2 wariantach:
		\begin{itemize}
			\item z prawdopodobieństwem P mutacji jednego (losowego) z parametrów
			\item z prawdopodobieństwem P mutacji każdego z parametrów \\
			(w szczególności mogą zmutować wszystkie)
		\end{itemize}
	Mutacja byłaby przeprowadzana losując jednostajnie ze zbioru elementów w zadanym promieniu od aktualnego (względnym w stosunku do zakresu). W obu wariantach promień mutacji miałby maleć z czasem by od eksploracji przejść do eksploatacji maximum lokalnego. 
	\item przegląd wyczerpujący hipersiatki jako metoda bazowa
\end{itemize}
%Algorytmy działałyby na siatce o zadanej gęstości [\textsl{parametry próbkowane %z względną gęstością np. zakres/50}]

% booster=gbtree, tree_method = approx
Trenowane parametry. Forma hipersiatki 
\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		nazwa parametu & zakres \\
		\hline
		\hline
		eta &  0.2, 0.3, 0.4, 0.5\\ % 0.3 = default
		\hline 
		min\_split\_loss \textit{gamma} & 0, 1, 2, 3\\ % 20 = extremly high
		\hline 
		max\_depth & 1, 2, 3 ... 16\\
	    \hline
		min\_child\_weight & 1, 2\\ % TODO float/int ?
		\hline 
		max\_delta\_step & 0, 1, 2\\
		\hline 
		subsample & 0.5, 0.6, 0.7, 0.8, 0.9, 1\\
		\hline
		colsample\_bytree &  0.6, 0.8, 1\\ % 1 = default
		\hline 
	\end{tabular}
\end{center}                         

\section{Funkcja celu}
Jako funkcję celu przybraliśmy \textit{Average Precision Recall} obliczając wartość funkcji celu jako średnią arytmetyczną skuteczności przypisania predykcji. Planujemy wykorzystać implementację z pakietu \textit{scikit-learn}.\\
Ta sama funkcja zostanie wykorzystana do oceny jakości finalnych wytrenowanych modeli na zbiorze testowym.

\section{Sposób mierzenia jakości rozwiązania}
Będziemy porównywać algorytm pod kątem czasu działania względem wyczerpującego przeglądu, analizując czy zysk z szybszego doboru parametrów jest wystarczająco duży, by go stosować dla różnych limitów przejrzanych kombinacji dla naszych algorytmów.
\\
Zbiór danych zostanie podzielony na 2 podzbiory - uczenia i testowy \textsl{uczący odpowiednio większy}. Na zbiorze testowym nie będziemy uczyć i podejmować decyzji - zostanie on wykorzystany do zmierzenia działania algorytmu po dobraniu wszystkich parametrów. Zbiór uczący będzie działać z k-krotną walidacją krzyżową lub podzielony na zbiór uczenia i weryfikacyjny.

\subsection{Weryfikacja rozwiązania na innych danych}
Końcową wersję naszego algorytmu heurystycznego planujemy przetestować przy użyciu dodatkowego zbioru danych dot. przewidywania bankructwa polskich firm, który analizowaliśmy w \href{https://github.com/przestaw/Data_Mining_Bancrupcy}{innym projekcie}.

\end{document}